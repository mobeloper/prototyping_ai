{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1522ca3b",
   "metadata": {},
   "source": [
    "# General Adversarial Networks\n",
    "\n",
    "Generative Adversarial Networks (GANs) are a type of deep learning model that is used for generating new, synthetic data that resembles existing data. GANs consist of two neural networks:\n",
    "\n",
    "1. Generator (G): This network takes a random noise vector as input and generates a synthetic data sample that is similar to the existing data.\n",
    "2. Discriminator (D): This network takes a data sample (either real or synthetic) as input and outputs a probability that the sample is real.\n",
    "\n",
    "The two networks are trained simultaneously, with the goal of improving the generator's ability to produce realistic data samples and the discriminator's ability to correctly classify real and synthetic data samples.\n",
    "\n",
    "<img src=\"../images/gan.png\" alt=\"GANs\" width=\"600\"/>\n",
    "\n",
    "\n",
    "source: https://tikz.net/gan/\n",
    "\n",
    "The training process works as follows:\n",
    "\n",
    "1. The generator takes a random noise vector as input and generates a synthetic data sample.\n",
    "2. The discriminator takes the synthetic data sample and outputs a probability that it is real.\n",
    "3. The discriminator also takes a real data sample and outputs a probability that it is real.\n",
    "4. The generator's goal is to produce synthetic data samples that are indistinguishable from real data samples, so it tries to maximize the probability that the discriminator will classify its output as real.\n",
    "5. The discriminator's goal is to correctly classify real and synthetic data samples, so it tries to minimize the probability that it will incorrectly classify a synthetic data sample as real.\n",
    "6. The generator and discriminator are updated iteratively, with the generator trying to improve its ability to produce realistic data samples and the discriminator trying to improve its ability to correctly classify real and synthetic data samples.\n",
    "\n",
    "GANs have several applications, including:\n",
    "\n",
    "1. Data augmentation: GANs can be used to generate new data samples that can be used to augment existing datasets, which can improve the performance of machine learning models.\n",
    "2. Data generation: GANs can be used to generate new data samples that are similar to existing data, which can be used to simulate real-world scenarios or to generate new data for testing and evaluation.\n",
    "3. Image and video synthesis: GANs can be used to generate new images and videos that are similar to existing ones, which can be used for applications such as video editing or image generation.\n",
    "4. Style transfer: GANs can be used to transfer the style of one image to another, which can be used for applications such as image editing or artistic rendering.\n",
    "\n",
    "GANs have several advantages, including:\n",
    "\n",
    "1. Ability to generate high-quality, realistic data samples.\n",
    "2. Ability to learn complex, non-linear relationships between the input data and the output data.\n",
    "3. Ability to generate new data samples that are similar to existing data, but not identical.\n",
    "\n",
    "However, GANs also have some limitations, including:\n",
    "\n",
    "1. Difficulty in training: GANs can be difficult to train, especially for large datasets or complex tasks.\n",
    "2. Mode collapse: GANs can suffer from mode collapse, which is a phenomenon where the generator produces a limited number of output samples that are similar to each other.\n",
    "3. Unstable training: GANs can be prone to unstable training, which can cause the generator and discriminator to diverge or the training process to fail.\n",
    "\n",
    "Overall, GANs are a powerful tool for generating new, synthetic data that resembles existing data, and have many applications in fields such as computer vision, natural language processing, and audio processing.\n",
    "\n",
    "Training a Conditional Generative Adversarial Network (cGAN) on the FashionMNIST dataset enables the generation of fashion images conditioned on specific clothing categories. Here's a step-by-step guide to implementing a cGAN using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad1756ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def7a5b7",
   "metadata": {},
   "source": [
    "## 2. Load and Preprocess the FashionMNIST Dataset**\n",
    "\n",
    "Load the dataset and apply necessary transformations.\n",
    "Define image transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81a5dad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(\n",
    "    root='./data', train=True, transform=transform, download=True\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fcd50b",
   "metadata": {},
   "source": [
    "## 3. Define the Generator and Discriminator Architectures\n",
    "\n",
    "In a cGAN, both the generator and discriminator are conditioned on additional information, such as class labels.\n",
    "\n",
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab8da8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, num_classes, img_shape):\n",
    "        super(Generator, self).__init__()\n",
    "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
    "        self.init_size = img_shape[1] // 4\n",
    "        self.l1 = nn.Sequential(nn.Linear(latent_dim + num_classes, 128 * self.init_size ** 2))\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, img_shape[0], 3, stride=1, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, labels):\n",
    "        gen_input = torch.cat((self.label_emb(labels), noise), -1)\n",
    "        out = self.l1(gen_input)\n",
    "        out = out.view(out.size(0), 128, self.init_size, self.init_size)\n",
    "        img = self.conv_blocks(out)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3c5e2a",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b43556a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_classes, img_shape):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(num_classes + int(np.prod(img_shape)), 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        d_in = torch.cat((img.view(img.size(0), -1), self.label_emb(labels)), -1)\n",
    "        validity = self.model(d_in)\n",
    "        return validity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce80fbf4",
   "metadata": {},
   "source": [
    "## 4. Initialize Models and Optimizers\n",
    "\n",
    "Set up the models, loss function, and optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe77ef1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSELoss()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_dim = 100\n",
    "num_classes = 10\n",
    "img_shape = (1, 28, 28)\n",
    "\n",
    "# Initialize models\n",
    "generator = Generator(latent_dim, num_classes, img_shape)\n",
    "discriminator = Discriminator(num_classes, img_shape)\n",
    "\n",
    "# Loss function\n",
    "adversarial_loss = nn.MSELoss()\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "generator.to(device)\n",
    "discriminator.to(device)\n",
    "adversarial_loss.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56190514",
   "metadata": {},
   "source": [
    "## 5. Training the cGAN\n",
    "\n",
    "Train the generator and discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2002545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [Batch 0/938] [D loss: 0.493446]\n",
      "[Epoch 0/200] [Batch 400/938] [D loss: 0.189300]\n",
      "[Epoch 0/200] [Batch 800/938] [D loss: 0.174199]\n",
      "[Epoch 1/200] [Batch 0/938] [D loss: 0.190361]\n",
      "[Epoch 1/200] [Batch 400/938] [D loss: 0.211259]\n",
      "[Epoch 1/200] [Batch 800/938] [D loss: 0.201509]\n",
      "[Epoch 2/200] [Batch 0/938] [D loss: 0.227705]\n",
      "[Epoch 2/200] [Batch 400/938] [D loss: 0.217840]\n",
      "[Epoch 2/200] [Batch 800/938] [D loss: 0.207462]\n",
      "[Epoch 3/200] [Batch 0/938] [D loss: 0.229227]\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "n_epochs = 200\n",
    "sample_interval = 400\n",
    "\n",
    "# Function to generate labels\n",
    "def generate_labels(n, num_classes):\n",
    "    return torch.randint(0, num_classes, (n,))\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    for i, (imgs, labels) in enumerate(train_loader):\n",
    "\n",
    "        batch_size = imgs.size(0)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = torch.ones((batch_size, 1), requires_grad=False).to(device)\n",
    "        fake = torch.zeros((batch_size, 1), requires_grad=False).to(device)\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Sample noise and labels as generator input\n",
    "        z = torch.randn((batch_size, latent_dim)).to(device)\n",
    "        gen_labels = generate_labels(batch_size, num_classes).to(device)\n",
    "\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(z, gen_labels)\n",
    "\n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        validity = discriminator(gen_imgs, gen_labels)\n",
    "        g_loss = adversarial_loss(validity, valid)\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Loss for real images\n",
    "        validity_real = discriminator(real_imgs, labels)\n",
    "        d_real_loss = adversarial_loss(validity_real, valid)\n",
    "\n",
    "        # Loss for fake images\n",
    "        validity_fake = discriminator(gen_imgs.detach(), gen_labels)\n",
    "        d_fake_loss = adversarial_loss(validity_fake, fake)\n",
    "\n",
    "        # Total discriminator loss\n",
    "        d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Print training progress\n",
    "        if i % sample_interval == 0:\n",
    "            print(f\"[Epoch {epoch}/{n_epochs}] [Batch {i}/{len(train_loader)}] \"\n",
    "                  f\"[D loss: {d_loss.item():.6f}]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cleanenv)",
   "language": "python",
   "name": "cleanenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
