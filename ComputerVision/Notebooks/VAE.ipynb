{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JxNa1oUXQt3G"
   },
   "source": [
    "# Variational Autoencoders\n",
    "\n",
    "\n",
    "## Autencoders\n",
    "\n",
    "Autoencoders are a type of neural network that is trained to reconstruct its input. They are often used for dimensionality reduction, anomaly detection, and generative modeling.\n",
    "\n",
    "The basic architecture of an autoencoder consists of two main components:\n",
    "\n",
    "Encoder: The encoder is a neural network that takes the input data and maps it to a lower-dimensional latent space. The encoder is typically a feedforward neural network with one or more hidden layers.\n",
    "Decoder: The decoder is a neural network that takes the latent representation and maps it back to the original input space. The decoder is also typically a feedforward neural network with one or more hidden layers.\n",
    "\n",
    "<img src=\"../images/ae.png\" alt=\"Autoencoder\" width=\"600\"/>\n",
    "\n",
    "## Variational Autoencoders (VAEs)\n",
    "\n",
    "Variational Autoencoders (VAEs) are a type of deep learning model that is used for unsupervised learning and dimensionality reduction. They are a combination of an encoder and a decoder, and are trained to learn a probabilistic mapping between the input data and a lower-dimensional latent space.\n",
    "\n",
    "The main goal of a VAE is to learn a probabilistic representation of the input data, which can be used for tasks such as:\n",
    "\n",
    "1. Dimensionality reduction: VAEs can be used to reduce the dimensionality of high-dimensional data, such as images or text, to a lower-dimensional representation that is easier to work with.\n",
    "2. Anomaly detection: VAEs can be used to detect anomalies or outliers in the data by identifying points that are farthest from the mean of the latent space.\n",
    "3. Generative modeling: VAEs can be used to generate new data samples that are similar to the training data, by sampling from the latent space and passing the samples through the decoder.\n",
    "4. Data imputation: VAEs can be used to impute missing values in the data by learning a probabilistic model of the data and using it to predict the missing values.\n",
    "\n",
    "The architecture of a VAE typically consists of two main components:\n",
    "\n",
    "<img src=\"../images/vae.png\" alt=\"Variational Autoencoder\" width=\"600\"/>\n",
    "\n",
    "\n",
    "1. Encoder: The encoder is a neural network that takes the input data and maps it to a lower-dimensional latent space. The encoder is trained to minimize the reconstruction error between the input data and the reconstructed data.\n",
    "2. Decoder: The decoder is a neural network that takes the latent representation and maps it back to the original input space. The decoder is also trained to minimize the reconstruction error between the input data and the reconstructed data.\n",
    "\n",
    "The key innovation of VAEs is the use of a probabilistic approach to learn the mapping between the input data and the latent space. Specifically, the encoder is trained to learn a probabilistic distribution over the latent space, and the decoder is trained to learn a probabilistic mapping from the latent space to the input space.\n",
    "\n",
    "VAEs have several advantages over other dimensionality reduction techniques, such as PCA or t-SNE. For example:\n",
    "\n",
    "1. VAEs can learn complex, non-linear relationships between the input data and the latent space.\n",
    "2. VAEs can learn to capture high-level features of the data, such as shapes or textures, rather than just low-level features such as edges or lines.\n",
    "3. VAEs can be used for both dimensionality reduction and generative modeling, making them a versatile tool for a wide range of applications.\n",
    "\n",
    "VAEs are a powerful tool for unsupervised learning and dimensionality reduction, and have been successfully applied to a wide range of applications, including computer vision, natural language processing, and recommender systems.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in /Users/harishkashyap/cleanenv/lib/python3.12/site-packages (0.20.1)\r\n",
      "Requirement already satisfied: numpy in /Users/harishkashyap/cleanenv/lib/python3.12/site-packages (from torchvision) (2.2.0)\r\n",
      "Requirement already satisfied: torch==2.5.1 in /Users/harishkashyap/cleanenv/lib/python3.12/site-packages (from torchvision) (2.5.1)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/harishkashyap/cleanenv/lib/python3.12/site-packages (from torchvision) (11.0.0)\r\n",
      "Requirement already satisfied: filelock in /Users/harishkashyap/cleanenv/lib/python3.12/site-packages (from torch==2.5.1->torchvision) (3.16.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/harishkashyap/cleanenv/lib/python3.12/site-packages (from torch==2.5.1->torchvision) (4.12.2)\r\n",
      "Requirement already satisfied: networkx in /Users/harishkashyap/cleanenv/lib/python3.12/site-packages (from torch==2.5.1->torchvision) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /Users/harishkashyap/cleanenv/lib/python3.12/site-packages (from torch==2.5.1->torchvision) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /Users/harishkashyap/cleanenv/lib/python3.12/site-packages (from torch==2.5.1->torchvision) (2024.12.0)\r\n",
      "Requirement already satisfied: setuptools in /Users/harishkashyap/cleanenv/lib/python3.12/site-packages (from torch==2.5.1->torchvision) (75.6.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/harishkashyap/cleanenv/lib/python3.12/site-packages (from torch==2.5.1->torchvision) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/harishkashyap/cleanenv/lib/python3.12/site-packages (from sympy==1.13.1->torch==2.5.1->torchvision) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/harishkashyap/cleanenv/lib/python3.12/site-packages (from jinja2->torch==2.5.1->torchvision) (3.0.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UYOF95Nd4q4r",
    "outputId": "0fa1a088-44e3-4a9e-9628-cbe931a6c64c"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qoLZ3iI2QWjQ"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J_GSv7fQQW1p"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WXE4vHyKQXJT"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "qN2QcbYiQXWm"
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Hyperparameters\n",
    "# -------------------------------\n",
    "batch_size = 128\n",
    "latent_dim = 20   # dimension of the latent space\n",
    "epochs = 5\n",
    "learning_rate = 1e-3\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q9ueJJL_QXiz"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nAkOirjfQXur"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "REMG14oUQX7B"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "LD4HKy7tQYHc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------------------------------\n",
    "# Dataset and Dataloader\n",
    "# -------------------------------\n",
    "transform = transforms.ToTensor()\n",
    "train_dataset = datasets.MNIST(root='.', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rKLsiIR2QYS7"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c23O3XeyQYel"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5L38mwolQjPP"
   },
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# VAE Model Definition\n",
    "# -------------------------------\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim=20):\n",
    "        super(VAE, self).__init__()\n",
    "        # Encoder: takes in [batch, 1, 28, 28], produces parameters of q(z|x)\n",
    "        # Flatten: 28 * 28 = 784\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(784, 400),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.fc_mu = nn.Linear(400, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(400, latent_dim)\n",
    "\n",
    "        # Decoder: takes in z and produces parameters of p(x|z)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 400),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(400, 784),\n",
    "            nn.Sigmoid()  # output pixel values between 0 and 1\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        # x: [batch, 1, 28, 28]\n",
    "        x = x.view(-1, 784)  # flatten\n",
    "        h = self.encoder(x)  # [batch, 400]\n",
    "        mu = self.fc_mu(h)   # [batch, latent_dim]\n",
    "        logvar = self.fc_logvar(h)  # [batch, latent_dim]\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        # reparameterization trick: z = mu + sigma * epsilon\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        epsilon = torch.randn_like(std)\n",
    "        z = mu + std * epsilon\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        # z: [batch, latent_dim]\n",
    "        x_recon = self.decoder(z) # [batch, 784]\n",
    "        x_recon = x_recon.view(-1, 1, 28, 28)\n",
    "        return x_recon\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_recon = self.decode(z)\n",
    "        return x_recon, mu, logvar\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ZegjNnUQm3Q"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "SDac6PchQnBq"
   },
   "outputs": [],
   "source": [
    "# Instantiate model and optimizer\n",
    "model = VAE(latent_dim=latent_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# -------------------------------\n",
    "# ELBO Loss Function\n",
    "# -------------------------------\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    # Reconstruction loss: binary cross-entropy\n",
    "    # Treats the reconstructed image as a Bernoulli distribution parameter\n",
    "    recon_loss = nn.functional.binary_cross_entropy(\n",
    "        recon_x.view(-1, 784),\n",
    "        x.view(-1, 784),\n",
    "        reduction='sum'\n",
    "    )\n",
    "\n",
    "    # KL Divergence: KL(q(z|x) || p(z)) =\n",
    "    # 0.5 * sum( exp(logvar) + mu^2 - 1 - logvar )\n",
    "    kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return recon_loss + kl_div\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21_0FbnnQp0V"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Cx7DRc2rQp8o"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 166.3181\n",
      "Epoch 2/5, Loss: 121.7623\n",
      "Epoch 3/5, Loss: 114.6449\n",
      "Epoch 4/5, Loss: 111.7185\n",
      "Epoch 5/5, Loss: 110.0456\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Training Loop\n",
    "# -------------------------------\n",
    "model.train()\n",
    "for epoch in range(1, epochs + 1):\n",
    "    total_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        recon_data, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_data, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch}/{epochs}, Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKEREIF7QsIv"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "o1TJNnGkQsR4"
   },
   "outputs": [],
   "source": [
    "\n",
    "# After training, we can sample from the model by sampling z ~ N(0,I) and decoding:\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(batch_size, latent_dim).to(device)\n",
    "    sample = model.decode(z)\n",
    "    # 'sample' now\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (cleanenv)",
   "language": "python",
   "name": "cleanenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
