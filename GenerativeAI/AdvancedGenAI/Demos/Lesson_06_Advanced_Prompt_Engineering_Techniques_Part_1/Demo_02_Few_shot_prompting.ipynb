{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33679ce2-9df6-4bc5-b875-953500b0cdc0",
   "metadata": {
    "id": "33679ce2-9df6-4bc5-b875-953500b0cdc0"
   },
   "source": [
    "# __Demo: Few-Shot Prompting with LangChain and OpenAI__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884ce215-54cf-4a8f-8177-9189423a412c",
   "metadata": {
    "id": "884ce215-54cf-4a8f-8177-9189423a412c"
   },
   "source": [
    "## __Steps to Perform:__\n",
    "Step 1: Set up the OpenAI API Key\n",
    "\n",
    "Step 2: Define a Function to Get Completion\n",
    "\n",
    "Step 3: Define Your Prompt\n",
    "\n",
    "Step 4: Test the Few-Shot Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17522620-a9f9-46af-9293-c81e63442bc1",
   "metadata": {
    "id": "17522620-a9f9-46af-9293-c81e63442bc1"
   },
   "source": [
    "### __Step 1: Set up the OpenAI API Key__\n",
    "\n",
    "- The code imports the necessary libraries.\n",
    "- The __os__ is used for interacting with the operating system, and __openai__ is the library required to work with OpenAI's API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39c86a56-9cb2-4759-aabf-5136dd059077",
   "metadata": {
    "id": "39c86a56-9cb2-4759-aabf-5136dd059077"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# from langchain.llms import OpenAI\n",
    "from langchain.llms import ChatOpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "#from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "#_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "#openai.api_key  = os.getenv('OPENAI_API_KEY')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e877b6ac-1a14-4535-b38a-2502f02abc33",
   "metadata": {
    "id": "e877b6ac-1a14-4535-b38a-2502f02abc33"
   },
   "source": [
    "### __Step 2: Initialize the langchain__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc464f8a-f318-4097-91c9-fd1fded34969",
   "metadata": {
    "id": "bc464f8a-f318-4097-91c9-fd1fded34969"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/voc/work/.local/lib/python3.10/site-packages/langchain/llms/openai.py:241: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "/voc/work/.local/lib/python3.10/site-packages/langchain/llms/openai.py:898: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize the LangChain OpenAI wrapper\n",
    "# llm = OpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "# llm = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5d4ecd-b78a-457a-8629-dc58d97c3eaf",
   "metadata": {
    "id": "2a5d4ecd-b78a-457a-8629-dc58d97c3eaf"
   },
   "source": [
    "### __Step 3: Define Your Prompts__\n",
    "- The prompt consists of several pieces of customer feedback, each followed by a classification (Positive, Negative, or Neutral).\n",
    "- These examples serve to teach the model what kind of output is expected.\n",
    "- The final feedback statement is the new instance for which the classification is sought."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de748602-cb16-4632-bd4a-39ea74cc2d10",
   "metadata": {
    "id": "de748602-cb16-4632-bd4a-39ea74cc2d10",
    "outputId": "e707b067-2db5-4254-c40f-896d17116694"
   },
   "outputs": [],
   "source": [
    "# Define the few-shot prompt template\n",
    "template = \"\"\"\n",
    "Feedback: \"I loved the quick service and friendly staff.\"\n",
    "Classification: Positive\n",
    "\n",
    "Feedback: \"The product did not meet my expectations.\"\n",
    "Classification: Negative\n",
    "\n",
    "Feedback: \"I am not sure if this is the right product for me.\"\n",
    "Classification: Neutral\n",
    "\n",
    "Feedback: \"{new_feedback}\"\n",
    "Classification:\n",
    "\"\"\"\n",
    "\n",
    "# Create the prompt template with input variables\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"new_feedback\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d09bb85e-e271-4480-b6b2-c15f86aebff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the LLM chain\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faef2946-fa43-4460-8319-0b5f6f0d3d83",
   "metadata": {},
   "source": [
    "### __Step 4: Test the Few-Shot Prompt__\n",
    "Provide a new feedback statement for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44bb82e1-c698-4622-8486-f9dc3311dcd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification: Mixed\n"
     ]
    }
   ],
   "source": [
    "# New feedback to classify\n",
    "new_feedback = \"Your customer support was helpful, but the issue took too long to resolve.\"\n",
    "\n",
    "# Run the chain\n",
    "response = llm_chain.run(new_feedback)\n",
    "print(\"Classification:\", response.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41311957-8999-431b-8196-03626a68c44c",
   "metadata": {
    "id": "41311957-8999-431b-8196-03626a68c44c"
   },
   "source": [
    "In this few-shot scenario, the model uses the provided examples to understand the context and criteria for classification, applying that understanding to classify new, unseen feedback. This is particularly useful for more subjective tasks like sentiment analysis, where context and nuance play significant roles."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
