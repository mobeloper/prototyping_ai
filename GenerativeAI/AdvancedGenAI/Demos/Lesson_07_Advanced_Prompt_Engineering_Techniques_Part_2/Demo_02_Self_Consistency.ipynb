{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7WOjV7vo9cT6",
   "metadata": {
    "id": "7WOjV7vo9cT6"
   },
   "source": [
    "# __Demo: Self-Consistency Prompting with LangChain and OpenAI__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZKzMpVFbOoWU",
   "metadata": {
    "id": "ZKzMpVFbOoWU"
   },
   "source": [
    "## __Steps to Perform:__\n",
    "\n",
    "Step 1: Set up the OpenAI API Key\n",
    "\n",
    "Step 2: Define the Self-Consistency Prompt Template\n",
    "\n",
    "Step 3: Create the Chain and Run the Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AfdqBdeKPnVX",
   "metadata": {
    "id": "AfdqBdeKPnVX"
   },
   "source": [
    "### __Step 1: Set up the OpenAI API Key__\n",
    "- Import the required libraries and set up the OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9a0ea85-80ba-4af5-bc64-73d74a28b57f",
   "metadata": {
    "id": "c9a0ea85-80ba-4af5-bc64-73d74a28b57f"
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "# import openai\n",
    "\n",
    "# from langchain.llms import OpenAI\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# from openai import OpenAI\n",
    "\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "# openai.api_key  = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "client = OpenAI(api_key=os.environ['OPENAI_API_KEY'])\n",
    "\n",
    "# Initialize the OpenAI model with temperature > 0 for varied outputs\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0.7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IPE1LxgOPSUo",
   "metadata": {
    "id": "IPE1LxgOPSUo"
   },
   "source": [
    "### __Step 2: Define the Self-Consistency Prompt Template__\n",
    "We use a prompt template to ask the model to generate multiple lines of reasoning for the given problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d0d44b2-14fd-45f0-b2cc-9ca440887758",
   "metadata": {
    "id": "7d0d44b2-14fd-45f0-b2cc-9ca440887758"
   },
   "outputs": [],
   "source": [
    "# Define a self-consistency prompt template\n",
    "template = \"\"\"\n",
    "Question: {question}\n",
    "\n",
    "Generate three different lines of reasoning to solve the question. Each line should approach the problem differently.\n",
    "\n",
    "Line of Reasoning 1:\n",
    "Line of Reasoning 2:\n",
    "Line of Reasoning 3:\n",
    "\n",
    "Based on these lines of reasoning, the most consistent answer is:\n",
    "\"\"\"\n",
    "\n",
    "# Create the LangChain prompt template\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Qo0MsRX5PFqN",
   "metadata": {
    "id": "Qo0MsRX5PFqN"
   },
   "source": [
    "### __Step 3: Create the Chain and Run the Prompt__\n",
    "We use LLMChain to execute the prompt and get responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47154330-0504-4524-b08e-832c479646b6",
   "metadata": {
    "id": "47154330-0504-4524-b08e-832c479646b6",
    "outputId": "52dd08c9-bc0c-4ced-cc2b-619e3a08fa68"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Response:\n",
      " Line of Reasoning 1:\n",
      "This reasoning is based purely on the standard unit of weight measurement. A pound is a unit of weight used in the imperial system, and is commonly used in the United States. We are given that the weight is 30 pounds. On the other hand, the feathers are simply stated to be 1000 in number. However, the question does not provide us with the weight of a single feather. Therefore, without further information, we can only compare the weight to the known quantity, which is the 30-pound weight.\n",
      "\n",
      "Line of Reasoning 2:\n",
      "This reasoning involves a bit of science. It's true that feathers are much lighter than most other objects. However, 1000 is a considerably large number. If each feather weighed just a bit, collectively, they could possibly outweigh the 30-pound weight. But, without the average weight of a feather, we can't make this calculation. So, based on the information given, we must assume that the 30-pound weight is heavier.\n",
      "\n",
      "Line of Reasoning 3:\n",
      "This reasoning is based on common sense and general knowledge about the world. We know that feathers are extremely light. Even a large number of feathers, like 1000, would be unlikely to weigh much. Compare this to a 30-pound weight, which is designed to be heavy and is commonly used for strength training. Based on everyday experience, we can reasonably infer that the 30-pound weight is heavier.\n",
      "\n",
      "Based on these lines of reasoning, the most consistent answer is: the 30-pound weight is heavier.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the LangChain LLMChain\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "# Define the question\n",
    "question = \"Which is heavier: 1000 feathers or a 30-pound weight?\"\n",
    "\n",
    "# Run the chain\n",
    "response = llm_chain.run(question)\n",
    "# response = llm.invoke([HumanMessage(content=question)])\n",
    "print(\"Final Response:\\n\", response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_5hsOXzWPufh",
   "metadata": {
    "id": "_5hsOXzWPufh"
   },
   "source": [
    "You have learned how to use LangChain and OpenAI to generate creative content from prompts. You have also gained experience using self-consistency prompting to guide the modelâ€™s output."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
