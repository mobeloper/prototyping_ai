{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "NId_WJGCdDAD",
   "metadata": {
    "id": "NId_WJGCdDAD"
   },
   "source": [
    "# **Demo: ROUGE Benchmark**\n",
    "\n",
    "This demo is designed to read a PDF file and a summary of that file, and then compute the ROUGE scores for the summary by comparing it with the original document. The ROUGE scores provide a measure of the quality of the summary.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "j0Il1Sz9dQcm",
   "metadata": {
    "id": "j0Il1Sz9dQcm"
   },
   "source": [
    "## **Steps to Perform:**\n",
    "\n",
    "\n",
    "*   Step 1: Import the Necessary Libraries\n",
    "*   Step 2: Read the PDF File\n",
    "*   Step 3: Define the Summary Function\n",
    "*   Step 4: Load the ROUGE Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EVIHyQY2eL8l",
   "metadata": {
    "id": "EVIHyQY2eL8l"
   },
   "source": [
    "### **Step 1: Import the Necessary Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d34e534-359a-486f-9811-627b3eaabb68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/site-packages (from rouge-score) (2.0.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/site-packages (from rouge-score) (3.8.1)\n",
      "Requirement already satisfied: numpy in ./.local/lib/python3.10/site-packages (from rouge-score) (1.24.3)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/site-packages (from nltk->rouge-score) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/site-packages (from nltk->rouge-score) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/site-packages (from nltk->rouge-score) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in ./.local/lib/python3.10/site-packages (from nltk->rouge-score) (4.67.1)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24936 sha256=3a6391fc7cbe83ab9f9918d8fb096ac65b5f4d70a143ecfb299f238e37c81161\n",
      "  Stored in directory: /voc/work/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: rouge-score\n",
      "Successfully installed rouge-score-0.1.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": ["!pip install rouge-score"]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd3ede8e-cab4-4c3c-898b-714eef43e576",
   "metadata": {
    "id": "fd3ede8e-cab4-4c3c-898b-714eef43e576"
   },
   "outputs": [],
   "source": ["from rouge_score import rouge_scorer\nimport PyPDF2\nfrom openai import OpenAI\n\nclient = OpenAI()\nimport pandas as pd"]
  },
  {
   "cell_type": "markdown",
   "id": "GqA0mAqXffCZ",
   "metadata": {
    "id": "GqA0mAqXffCZ"
   },
   "source": [
    "## **Step 2: Read the PDF File**\n",
    "\n",
    "*   Open the PDF file.\n",
    "*   Create a **PdfReader** object for the PDF file.\n",
    "*   Extract the text from each page of the PDF and concatenate it into a single string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f24a902f-c255-4dc0-9047-39cd6550aa44",
   "metadata": {
    "id": "f24a902f-c255-4dc0-9047-39cd6550aa44"
   },
   "outputs": [],
   "source": ["# Read the PDF file\npdf_file = open('arxiv_impact_of_GENAI.pdf', 'rb')\npdf_reader = PyPDF2.PdfReader(pdf_file)\nnum_pages = len(pdf_reader.pages)\ndocument_text = \"\"\nfor page in range(num_pages):\n    document_text += pdf_reader.pages[page].extract_text()"]
  },
  {
   "cell_type": "markdown",
   "id": "ZLcKO7C8fx4O",
   "metadata": {
    "id": "ZLcKO7C8fx4O"
   },
   "source": [
    "## **Step 3: Define the Summary Function**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4Awcxb2Yf-o0",
   "metadata": {
    "id": "4Awcxb2Yf-o0"
   },
   "outputs": [],
   "source": ["def summarize_text(text, model=\"gpt-3.5-turbo\"):\n    truncated_text = text[:3000]  # Adjust for token limits\n    response = client.chat.completions.create(model=model,\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant that summarizes text.\"},\n        {\"role\": \"user\", \"content\": f\"Summarize the following text: {truncated_text}\"}\n    ])\n    return response.choices[0].message.content\n\nsummary_text = summarize_text(document_text)"]
  },
  {
   "cell_type": "markdown",
   "id": "96UJWz4Ng5ye",
   "metadata": {
    "id": "96UJWz4Ng5ye"
   },
   "source": [
    "## **Step 4: Load the ROUGE metric**\n",
    "\n",
    "*   Load the ROUGE metric.\n",
    "*   Compute the ROUGE scores for the summary.\n",
    "*   Print the scores.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "Hdj1QFyBhM6W",
   "metadata": {
    "id": "Hdj1QFyBhM6W"
   },
   "outputs": [],
   "source": ["# Compute ROUGE scores using rouge-score\nscorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\nscores = scorer.score(document_text, summary_text)\n"]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd311cca-bfbc-4d90-a0af-1c049c2f67d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Scores DataFrame:\n",
      "         Precision    Recall  F1-Score\n",
      "rouge1   0.943396  0.014599  0.028752\n",
      "rouge2   0.600000  0.009198  0.018119\n",
      "rougeL   0.716981  0.011095  0.021852\n"
     ]
    }
   ],
   "source": ["# Convert scores to a DataFrame\nscores_df = pd.DataFrame(scores).T\nscores_df.columns = ['Precision', 'Recall', 'F1-Score']\n\n# Print the scores\nprint(\"ROUGE Scores DataFrame:\\n\", scores_df)"]
  },
  {
   "cell_type": "markdown",
   "id": "f0e4af97-77a7-450b-8acf-43f7e193cb20",
   "metadata": {},
   "source": [
    "This approach uses the rouge-score library to calculate ROUGE scores. The output will be a DataFrame showing precision, recall, and F1 scores for rouge1, rouge2, and rougeL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163a6d3d-979a-4082-a8ab-979654dfeb4e",
   "metadata": {},
   "source": [
    "### **Conclusion**\n",
    "\n",
    "The ROUGE score output shows the F-measure for different versions of the ROUGE metric: ROUGE-1, ROUGE-2, and ROUGE-L. These scores provide a measure of how well the summary matches the reference document. The higher the score (closer to 1), the better the match between the summary and the original text."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 [3.10]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
