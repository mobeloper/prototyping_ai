{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Exercises: Understanding and Extending Bag of Words\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Modify the Stopword List**\n",
        "- Add custom words to the stopword list (e.g., domain-specific terms such as \"cat\", \"dog\", \"squirrel\", etc.).\n",
        "- Remove certain words from the default stopword list.\n",
        "- Observe how these changes affect the vocabulary and BoW representations.\n",
        "\n",
        "**Questions to Explore:**\n",
        "- How does the size of the vocabulary change when stopwords are modified?\n",
        "- Do certain documents now share more or fewer words in common?\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Implement N-grams**\n",
        "- Extend the `preprocess` function to include **bigrams** (n=2) or **trigrams** (n=3) instead of individual words.\n",
        "- Update the BoW generation to account for these n-grams.\n",
        "\n",
        "**Example:**\n",
        "- Original Text: `\"The cat sat on the mat.\"`\n",
        "- Vocabulary with Bigrams: `['the cat', 'cat sat', 'sat on', 'on the', 'the mat']`\n",
        "\n",
        "**Questions to Explore:**\n",
        "- How does the vocabulary size change when using n-grams?\n",
        "- Does the BoW representation become more meaningful or less meaningful?\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Normalize the BoW Representation**\n",
        "- Modify the `create_bow` function to normalize the BoW vectors by the total word count in each document.\n",
        "- This converts raw frequencies to relative frequencies.\n",
        "\n",
        "**Questions to Explore:**\n",
        "- How does normalization affect the BoW representations for documents with different lengths?\n",
        "- Is normalization helpful when comparing documents?\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Visualize the Vocabulary**\n",
        "- Use a bar chart to plot the most frequent words across all documents.\n",
        "- Highlight words excluded by the stopword list.\n",
        "\n",
        "**Questions to Explore:**\n",
        "- Which words dominate the vocabulary?\n",
        "- How does the stopword list affect word frequency distributions?\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Extend the Corpus**\n",
        "- Use a larger, more diverse corpus (e.g., a collection of news articles or book excerpts).\n",
        "- Generate the vocabulary and BoW representations for this extended dataset.\n",
        "\n",
        "**Questions to Explore:**\n",
        "- How does the size of the vocabulary grow with a larger corpus?\n",
        "- Are there new challenges with handling a larger vocabulary?\n",
        "\n",
        "---\n",
        "\n",
        "### **6. Experiment with Token Filters**\n",
        "- Update the `preprocess` function to include additional filters:\n",
        "  - **Word length**: Exclude words shorter than 3 characters.\n",
        "  - **Frequency threshold**: Exclude words appearing in less than 2 documents.\n",
        "\n",
        "**Questions to Explore:**\n",
        "- How do these filters reduce the size of the vocabulary?\n",
        "- Do the BoW representations lose or retain meaningful information?\n",
        "\n",
        "---\n",
        "\n",
        "### **7. Compare BoW with Term Frequency (TF)**\n",
        "- Modify the `create_bow` function to calculate **term frequency (TF)** instead of raw word counts.\n",
        "- TF normalizes word counts by the total number of words in the document.\n",
        "\n",
        "**Questions to Explore:**\n",
        "- How do the BoW representations differ when using raw counts vs. term frequency?\n",
        "- Which representation is more useful for comparing documents?\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Xas3N4mcxwE-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5v_3SosmsZt5"
      },
      "outputs": [],
      "source": []
    }
  ]
}