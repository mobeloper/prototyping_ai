{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# GloVe: Global Vectors for Word Representation\n",
        "\n",
        "In this notebook, we will explore **GloVe**, a technique for generating word embeddings. GloVe uses a co-occurrence matrix to capture word relationships and trains embeddings by minimizing a reconstruction loss.\n",
        "\n",
        "---\n",
        "\n",
        "## Objectives\n",
        "By the end of this notebook, you will:\n",
        "1. Understand how a co-occurrence matrix is created and used to capture contextual information.\n",
        "2. Learn how to train word embeddings using GloVe.\n",
        "3. Implement and use GloVe embeddings to find similar words.\n",
        "\n",
        "---\n",
        "\n",
        "## What is GloVe?\n",
        "GloVe (Global Vectors for Word Representation) is a method for learning word embeddings based on the co-occurrence of words in a corpus. Unlike Word2Vec, which uses local context windows, GloVe incorporates global statistics of word co-occurrence.\n",
        "\n",
        "### Key Features:\n",
        "- Uses a co-occurrence matrix to capture the number of times words appear together in a context window.\n",
        "- Learns word embeddings by factorizing this matrix and minimizing the difference between observed and predicted log co-occurrence counts.\n",
        "\n",
        "---\n",
        "\n",
        "## Why Use GloVe?\n",
        "Word embeddings generated by GloVe have several advantages:\n",
        "1. Capture semantic and syntactic relationships between words.\n",
        "2. Perform well on word similarity and analogy tasks.\n",
        "3. Provide interpretable embeddings useful in downstream NLP applications.\n"
      ],
      "metadata": {
        "id": "XLG8fkMdS_v1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preliminaries: Libraries and Data Preparation\n",
        "\n",
        "We will:\n",
        "1. Use `numpy` and `scipy.sparse` for matrix operations.\n",
        "2. Define a small corpus of sentences for training.\n",
        "3. Create a co-occurrence matrix to capture word relationships in the corpus."
      ],
      "metadata": {
        "id": "cvARNtteTGv1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Z4aG9AW3S34o"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy import sparse\n",
        "\n",
        "# Our sample corpus\n",
        "corpus = [\n",
        "    ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog'],\n",
        "    ['the', 'five', 'boxing', 'wizards', 'jump', 'quickly'],\n",
        "    ['pack', 'my', 'box', 'with', 'five', 'dozen', 'liquor', 'jugs'],\n",
        "    ['she', 'sells', 'seashells', 'by', 'the', 'seashore'],\n",
        "    ['how', 'much', 'wood', 'would', 'a', 'woodchuck', 'chuck', 'if', 'a', 'woodchuck', 'could', 'chuck', 'wood'],\n",
        "    ['peter', 'piper', 'picked', 'a', 'peck', 'of', 'pickled', 'peppers'],\n",
        "    ['a', 'peck', 'of', 'pickled', 'peppers', 'peter', 'piper', 'picked'],\n",
        "    ['to', 'be', 'or', 'not', 'to', 'be', 'that', 'is', 'the', 'question'],\n",
        "    ['all', 'the', 'world', 'is', 'a', 'stage', 'and', 'all', 'the', 'men', 'and', 'women', 'merely', 'players'],\n",
        "    ['friends', 'romans', 'countrymen', 'lend', 'me', 'your', 'ears'],\n",
        "    ['romeo', 'romeo', 'wherefore', 'art', 'thou', 'romeo'],\n",
        "    ['the', 'cat', 'sat', 'on', 'the', 'mat'],\n",
        "    ['the', 'dog', 'chased', 'the', 'cat'],\n",
        "    ['the', 'mat', 'was', 'on', 'the', 'floor'],\n",
        "    ['the', 'sun', 'shines', 'bright', 'in', 'the', 'blue', 'sky'],\n",
        "    ['rain', 'drops', 'keep', 'falling', 'on', 'my', 'head'],\n",
        "    ['in', 'a', 'hole', 'in', 'the', 'ground', 'there', 'lived', 'a', 'hobbit'],\n",
        "    ['it', 'was', 'the', 'best', 'of', 'times', 'it', 'was', 'the', 'worst', 'of', 'times'],\n",
        "    ['call', 'me', 'ishmael'],\n",
        "    ['it', 'is', 'a', 'truth', 'universally', 'acknowledged', 'that', 'a', 'single', 'man', 'in', 'possession', 'of', 'a', 'good', 'fortune', 'must', 'be', 'in', 'want', 'of', 'a', 'wife'],\n",
        "    ['happy', 'families', 'are', 'all', 'alike', 'every', 'unhappy', 'family', 'is', 'unhappy', 'in', 'its', 'own', 'way'],\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Create the Co-occurrence Matrix\n",
        "\n",
        "The co-occurrence matrix represents the number of times words appear together within a context window. Each row and column corresponds to a word in the vocabulary.\n",
        "\n",
        "### Parameters:\n",
        "- **Window Size**: Determines how many words to the left and right are considered as context.\n",
        "- **Vocab**: The set of unique words in the corpus.\n"
      ],
      "metadata": {
        "id": "5OIUXaPpTSBq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_co_occurrence_matrix(corpus, window_size=2):\n",
        "    # Create vocabulary and mappings\n",
        "    vocab = sorted(set(word for sentence in corpus for word in sentence))\n",
        "    word_to_id = {word: i for i, word in enumerate(vocab)}\n",
        "\n",
        "    # Initialize the co-occurrence matrix\n",
        "    co_occurrence = sparse.lil_matrix((len(vocab), len(vocab)))\n",
        "\n",
        "    # Populate the matrix\n",
        "    for sentence in corpus:\n",
        "        for i, word in enumerate(sentence):\n",
        "            left_context = max(0, i - window_size)\n",
        "            right_context = min(len(sentence), i + window_size + 1)\n",
        "            for j in range(left_context, right_context):\n",
        "                if i != j:\n",
        "                    co_occurrence[word_to_id[word], word_to_id[sentence[j]]] += 1\n",
        "\n",
        "    return co_occurrence, word_to_id\n",
        "\n",
        "# Create co-occurrence matrix\n",
        "co_occurrence, word_to_id = create_co_occurrence_matrix(corpus)\n",
        "\n",
        "print(\"Vocabulary:\", list(word_to_id.keys()))\n",
        "print(\"Co-occurrence matrix shape:\", co_occurrence.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k274LMCjTJ3F",
        "outputId": "9525ecbb-790d-4f21-86e0-f7821bfe11cd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['a', 'acknowledged', 'alike', 'all', 'and', 'are', 'art', 'be', 'best', 'blue', 'box', 'boxing', 'bright', 'brown', 'by', 'call', 'cat', 'chased', 'chuck', 'could', 'countrymen', 'dog', 'dozen', 'drops', 'ears', 'every', 'falling', 'families', 'family', 'five', 'floor', 'fortune', 'fox', 'friends', 'good', 'ground', 'happy', 'head', 'hobbit', 'hole', 'how', 'if', 'in', 'is', 'ishmael', 'it', 'its', 'jugs', 'jump', 'jumps', 'keep', 'lazy', 'lend', 'liquor', 'lived', 'man', 'mat', 'me', 'men', 'merely', 'much', 'must', 'my', 'not', 'of', 'on', 'or', 'over', 'own', 'pack', 'peck', 'peppers', 'peter', 'picked', 'pickled', 'piper', 'players', 'possession', 'question', 'quick', 'quickly', 'rain', 'romans', 'romeo', 'sat', 'seashells', 'seashore', 'sells', 'she', 'shines', 'single', 'sky', 'stage', 'sun', 'that', 'the', 'there', 'thou', 'times', 'to', 'truth', 'unhappy', 'universally', 'want', 'was', 'way', 'wherefore', 'wife', 'with', 'wizards', 'women', 'wood', 'woodchuck', 'world', 'worst', 'would', 'your']\n",
            "Co-occurrence matrix shape: (117, 117)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Train the GloVe Model\n",
        "\n",
        "We will train word embeddings using GloVe by minimizing the difference between the log of observed co-occurrence counts and predicted values.\n",
        "\n",
        "### Parameters:\n",
        "- **Vector Size**: Dimensionality of the word embeddings.\n",
        "- **Iterations**: Number of training iterations.\n",
        "- **Learning Rate**: Step size for updates during training.\n"
      ],
      "metadata": {
        "id": "9PwVoDoCTaEj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_glove(co_occurrence, word_to_id, vector_size=50, iterations=50, learning_rate=0.05):\n",
        "    vocab_size = len(word_to_id)\n",
        "    X = co_occurrence.tocsr()\n",
        "\n",
        "    # Apply logarithm to non-zero elements only\n",
        "    X_log = X.copy()\n",
        "    X_log.data = np.log(X_log.data)\n",
        "\n",
        "    # Initialize word vectors\n",
        "    W = np.random.randn(vocab_size, vector_size) / np.sqrt(vector_size)\n",
        "    W_context = np.random.randn(vocab_size, vector_size) / np.sqrt(vector_size)\n",
        "\n",
        "    for _ in range(iterations):\n",
        "        for i in range(vocab_size):\n",
        "            J = X[i].nonzero()[1]  # Get indices of non-zero elements\n",
        "\n",
        "            error = W[i].dot(W_context[J].T) - X_log[i, J].A.ravel()\n",
        "            grad = error.dot(W_context[J])\n",
        "\n",
        "            W[i] -= learning_rate * grad\n",
        "            W_context[J] -= learning_rate * np.outer(error, W[i])\n",
        "\n",
        "    # Final word vectors are the sum of W and W_context\n",
        "    return W + W_context\n",
        "\n",
        "\n",
        "# Train the model\n",
        "word_vectors = train_glove(co_occurrence, word_to_id, vector_size=20, iterations=100)\n",
        "print(\"Word vectors trained successfully!\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYV2SaqBTYCm",
        "outputId": "283b460f-8af1-41d1-f7a7-95009b60d006"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word vectors trained successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Find Similar Words\n",
        "\n",
        "Using the trained embeddings, we can find words that are most similar to a given word by computing cosine similarity.\n",
        "\n",
        "### Example:\n",
        "Query for \"quick\" might return \"fast\", \"speedy\", and other similar terms.\n"
      ],
      "metadata": {
        "id": "3uFGE4sJWGiQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_similar_words(word, word_to_id, word_vectors, top_n=3):\n",
        "    if word not in word_to_id:\n",
        "        return \"Word not in vocabulary\"\n",
        "\n",
        "    word_id = word_to_id[word]\n",
        "    word_vector = word_vectors[word_id]\n",
        "\n",
        "    similarities = word_vectors.dot(word_vector) / (np.linalg.norm(word_vectors, axis=1) * np.linalg.norm(word_vector))\n",
        "    most_similar = np.argsort(similarities)[::-1][1:top_n+1]\n",
        "\n",
        "    return [(list(word_to_id.keys())[list(word_to_id.values()).index(i)], similarities[i]) for i in most_similar]\n",
        "\n",
        "# Example: Find similar words\n",
        "\n",
        "print(\"\\nWords similar to 'quick':\")\n",
        "print(find_similar_words('quick', word_to_id, word_vectors))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoMKhkpPTeOM",
        "outputId": "a955d568-6af1-4242-d7db-156f6c2284c9"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Words similar to 'quick':\n",
            "[('women', 0.44342990320616027), ('dozen', 0.4300459737639469), ('quickly', 0.39493677590245657)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keep in mind that these results are based solely on the very small corpus we defined earlier. Results would be much better if the model were trained on, say, a publicly available dataset."
      ],
      "metadata": {
        "id": "zycV-HEiXtPc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercises: Hands-on Practice\n",
        "\n",
        "1. **Experiment with Window Size**:\n",
        "   Modify the `window_size` parameter when creating the co-occurrence matrix. Observe how it affects the embeddings.\n",
        "\n",
        "2. **Custom Corpus**:\n",
        "   Train GloVe on a different corpus, such as your own set of sentences or a publicly available dataset.\n",
        "\n",
        "3. **Explore Similar Words**:\n",
        "   Extend the `find_similar_words` function to list the top 10 most similar words for multiple queries.\n",
        "\n",
        "4. **Hyperparameter Tuning**:\n",
        "   Experiment with different values for `vector_size`, `iterations`, and `learning_rate`. Observe the impact on the quality of word embeddings.\n"
      ],
      "metadata": {
        "id": "gh_-exafXf3-"
      }
    }
  ]
}