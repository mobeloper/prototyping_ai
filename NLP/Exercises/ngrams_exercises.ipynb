{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Exercises: Understanding and Extending N-Gram Language Models\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Experiment with Different N-Gram Sizes**\n",
        "- Train the model with **different values of `n`** (e.g., 1 for unigrams, 2 for bigrams, 3 for trigrams).\n",
        "- Compare the quality of the generated text and the next word predictions for each model.\n",
        "\n",
        "**Questions to Explore:**\n",
        "- How does increasing `n` affect the coherence of the generated text?\n",
        "- Does a higher `n` capture context better, or does it overfit the training text?\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Extend the Corpus**\n",
        "- Use a larger and more diverse corpus, such as a collection of news articles, books, or Wikipedia text.\n",
        "- Train the model on this extended dataset and observe how the predictions and generated text improve.\n",
        "\n",
        "**Questions to Explore:**\n",
        "- How does the diversity of the corpus affect the model's ability to generate coherent sentences?\n",
        "- Does the model generalize better with a larger corpus?\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Add Smoothing**\n",
        "- Modify the `train` method to implement **additive smoothing** (e.g., Laplace smoothing).\n",
        "- This ensures that the model assigns a small nonzero probability to unseen n-grams.\n",
        "\n",
        "**Questions to Explore:**\n",
        "- How does smoothing affect next word predictions for rare or unseen n-grams?\n",
        "- Is the generated text more diverse after adding smoothing?\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Generate Text with Random Seeds**\n",
        "- Use randomly chosen seed words from the corpus to generate text.\n",
        "- Analyze the variety and coherence of the output for different seeds.\n",
        "\n",
        "**Questions to Explore:**\n",
        "- Are some seed words more likely to produce coherent text than others?\n",
        "- Does the model produce repetitive sequences for certain seeds?\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Evaluate with Perplexity**\n",
        "- Implement a method to calculate **perplexity**, a measure of how well the model predicts a test set.\n",
        "- Evaluate the model’s perplexity on held-out data.\n",
        "\n",
        "**Questions to Explore:**\n",
        "- How does perplexity vary with different values of `n`?\n",
        "- Does perplexity correlate with the perceived quality of generated text?\n",
        "\n",
        "---\n",
        "\n",
        "### **6. Predict Multiple Next Words**\n",
        "- Extend the `generate_next_word` method to predict the top `k` most likely next words.\n",
        "- Generate text by choosing from these top candidates randomly.\n",
        "\n",
        "**Questions to Explore:**\n",
        "- Does allowing multiple next words improve the diversity of the generated text?\n",
        "- How does the generated text quality change with larger values of `k`?\n",
        "\n",
        "---\n",
        "\n",
        "### **7. Handle Case Sensitivity**\n",
        "- Update the preprocessing step to handle case sensitivity more intelligently.\n",
        "- For example, treat `The` and `the` as the same word during training but preserve case in the output.\n",
        "\n",
        "**Questions to Explore:**\n",
        "- How does case normalization affect the model’s predictions and vocabulary size?\n",
        "- Is case information necessary for certain types of text (e.g., proper nouns)?\n",
        "\n",
        "---\n",
        "\n",
        "### **8. Visualize N-Gram Frequencies**\n",
        "- Plot a bar chart showing the top 10 most frequent n-grams in the corpus.\n",
        "- Highlight differences in frequency distributions for different values of `n`.\n",
        "\n",
        "**Questions to Explore:**\n",
        "- What are the most frequent n-grams, and do they make sense given the corpus?\n",
        "- How does the distribution change as `n` increases?\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "JxVgxBIqzUeo"
      }
    }
  ]
}