{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Building a Simple GPT Model from Scratch with PyTorch\n",
        "\n",
        "In this lesson, we will create a simplified version of the Generative Pre-trained Transformer (GPT) model using PyTorch. GPT models are powerful tools for generating coherent and meaningful text based on an initial input.\n",
        "\n",
        "## Objectives:\n",
        "By the end of this notebook, you will be able to:\n",
        "1. Understand the architecture of a GPT-like model.\n",
        "2. Implement the key components of the GPT architecture.\n",
        "3. Train the model on a toy dataset and evaluate its performance.\n",
        "4. Generate text using the trained model.\n",
        "\n",
        "## What is GPT?\n",
        "GPT stands for Generative Pre-trained Transformer. It is a type of language model that uses transformer layers to predict the next token in a sequence. This autoregressive property allows GPT models to generate high-quality text, making them suitable for applications such as:\n",
        "- Text completion\n",
        "- Chatbots\n",
        "- Text summarization\n",
        "- Translation\n",
        "\n",
        "In this notebook, we simplify the GPT architecture to focus on understanding the core concepts.\n"
      ],
      "metadata": {
        "id": "9ij7tNi9zmms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preliminaries: Libraries and Dependencies\n",
        "\n",
        "Before diving into the implementation, let’s import the necessary libraries and briefly discuss their roles:\n",
        "- **torch**: Core PyTorch library for building models and performing tensor computations.\n",
        "- **torch.nn**: Contains prebuilt modules and layers for constructing neural networks.\n",
        "- **torch.optim**: Provides optimization algorithms for training models.\n",
        "- **torch.nn.functional**: Contains utility functions for common operations like activation functions and loss computations.\n",
        "\n",
        "If you haven't installed PyTorch yet, follow the installation guide at https://pytorch.org/get-started/locally/.\n",
        "\n",
        "Let’s proceed with importing the required libraries.\n"
      ],
      "metadata": {
        "id": "yg15AetZzuWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "Dogx_ND9tVPz"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Key Components of the GPT Model\n",
        "\n",
        "The GPT model consists of the following key components:\n",
        "\n",
        "## 1. **Token Embeddings**\n",
        "Tokens are numerical representations of words or subwords. The token embedding layer converts these tokens into dense vector representations that the model can process.\n",
        "\n",
        "## 2. **Positional Embeddings**\n",
        "Transformers do not inherently understand the order of tokens in a sequence. Positional embeddings are added to token embeddings to encode sequence order information.\n",
        "\n",
        "## 3. **Transformer Layers**\n",
        "Transformer layers are the core building blocks of GPT models. Each layer contains:\n",
        "- **Self-attention mechanism**: Helps the model focus on relevant parts of the sequence.\n",
        "- **Feedforward neural network**: Processes the outputs of the self-attention mechanism.\n",
        "\n",
        "## 4. **Output Layer**\n",
        "The output layer generates logits (unnormalized scores) for predicting the next token in the sequence. These logits are passed through a softmax function to produce probabilities.\n",
        "\n",
        "In the next section, we will implement these components step by step in the `SimpleGPT` class.\n"
      ],
      "metadata": {
        "id": "8l995EB80MZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleGPT(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, num_heads, num_layers, max_seq_length):\n",
        "        super(SimpleGPT, self).__init__()\n",
        "        # Token and positional embeddings\n",
        "        self.token_embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.position_embedding = nn.Embedding(max_seq_length, embed_size)\n",
        "\n",
        "        # Transformer layers\n",
        "        self.transformer_layers = nn.ModuleList([\n",
        "            nn.TransformerEncoderLayer(d_model=embed_size, nhead=num_heads)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Output layer\n",
        "        self.output_layer = nn.Linear(embed_size, vocab_size)\n",
        "        self.max_seq_length = max_seq_length\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_length = x.size(1)\n",
        "        # Token embeddings\n",
        "        token_embeds = self.token_embedding(x)\n",
        "        # Positional embeddings\n",
        "        positions = torch.arange(0, seq_length).unsqueeze(0).repeat(x.size(0), 1).to(x.device)\n",
        "        position_embeds = self.position_embedding(positions)\n",
        "        # Combine embeddings\n",
        "        x = token_embeds + position_embeds\n",
        "        # Apply transformer layers\n",
        "        for layer in self.transformer_layers:\n",
        "            x = layer(x)\n",
        "        # Output logits\n",
        "        logits = self.output_layer(x)\n",
        "        return logits\n",
        "\n",
        "    def generate(self, start_tokens, max_length, temperature=1.0):\n",
        "        self.eval()\n",
        "        current_seq = start_tokens\n",
        "        with torch.no_grad():\n",
        "            for _ in range(max_length):\n",
        "                if len(current_seq) > self.max_seq_length:\n",
        "                    current_seq = current_seq[-self.max_seq_length:]\n",
        "                inputs = torch.tensor(current_seq).unsqueeze(0).to(next(self.parameters()).device)\n",
        "                logits = self(inputs)\n",
        "                next_token_logits = logits[0, -1, :] / temperature\n",
        "                next_token = torch.multinomial(F.softmax(next_token_logits, dim=-1), num_samples=1).item()\n",
        "                current_seq.append(next_token)\n",
        "                if next_token == 1:  # Assuming 1 is the EOS token\n",
        "                    break\n",
        "        return current_seq"
      ],
      "metadata": {
        "id": "NWigPJ2Ez9AG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the GPT Model\n",
        "\n",
        "Once we define the architecture of the model, the next step is to train it on a dataset. Training involves:\n",
        "1. Feeding a sequence of tokens to the model.\n",
        "2. Comparing the model's predictions with the ground truth (target tokens).\n",
        "3. Backpropagating the loss to adjust the model's weights.\n",
        "\n",
        "### Key Concepts in Training:\n",
        "- **Loss Function**: We use Cross-Entropy Loss to measure how well the predicted token probabilities match the target tokens.\n",
        "- **Optimizer**: Adam optimizer is used to adjust the model's weights and biases during training.\n",
        "- **Batch Processing**: Training on multiple sequences at a time for efficiency.\n",
        "\n",
        "## Alphabet Sequence Dataset\n",
        "\n",
        "To make the training process meaningful, we will train the model on sequences of letters from the English alphabet.\n",
        "\n",
        "## Example:\n",
        "- **Input**: `A B C D`\n",
        "- **Target**: `B C D E`\n",
        "\n",
        "The model learns to predict the next letter in a sequence.\n",
        "\n",
        "### Tokenization\n",
        "Since the model operates on numerical data, we need to map each letter to a unique token ID.\n",
        "For example:\n",
        "- `A -> 0`\n",
        "- `B -> 1`\n",
        "- `C -> 2`\n",
        "\n",
        "At the end of training, we will map the generated token IDs back to letters to evaluate the model's performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "YKKZy9Yp0o9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters\n",
        "vocab_size = 26\n",
        "embed_size = 128\n",
        "num_heads = 4\n",
        "num_layers = 2\n",
        "max_seq_length = 10\n",
        "\n",
        "# Instantiate the model\n",
        "model = SimpleGPT(vocab_size, embed_size, num_heads, num_layers, max_seq_length)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Tokenize a repeating alphabet sequence\n",
        "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
        "token_to_id = {ch: i for i, ch in enumerate(alphabet)}\n",
        "id_to_token = {i: ch for ch, i in token_to_id.items()}\n",
        "\n",
        "# Generate dataset\n",
        "def generate_text_data(batch_size, seq_length):\n",
        "    data = []\n",
        "    for _ in range(batch_size):\n",
        "        start = torch.randint(0, len(alphabet) - seq_length - 1, (1,)).item()\n",
        "        sequence = [token_to_id[ch] for ch in alphabet[start : start + seq_length + 1]]\n",
        "        data.append(sequence)\n",
        "    return torch.tensor(data)\n",
        "\n",
        "# Training loop using alphabet sequences\n",
        "for epoch in range(5):  # Number of epochs\n",
        "    for _ in range(10):  # Number of batches\n",
        "        data = generate_text_data(batch_size=8, seq_length=max_seq_length-1)\n",
        "        inputs, targets = data[:, :-1], data[:, 1:]  # Split into inputs and targets\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(inputs)\n",
        "        loss = criterion(logits.reshape(-1, vocab_size), targets.reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pleaMOCM0d-C",
        "outputId": "b434e69a-cbb9-4953-c173-5e74861fe7a5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.17410090565681458\n",
            "Epoch 2, Loss: 0.0467846542596817\n",
            "Epoch 3, Loss: 0.026736455038189888\n",
            "Epoch 4, Loss: 0.020729782059788704\n",
            "Epoch 5, Loss: 0.013821137137711048\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Generation with the GPT Model\n",
        "\n",
        "Once trained, the model can generate text using its `generate` method. The process involves:\n",
        "1. Starting with an initial sequence of tokens (called seed tokens).\n",
        "2. Feeding the sequence to the model to predict the next token.\n",
        "3. Sampling the next token from the predicted probabilities.\n",
        "4. Appending the new token to the sequence and repeating the process.\n",
        "\n",
        "### Important Parameters:\n",
        "- **max_length**: The maximum length of the generated sequence.\n",
        "- **temperature**: Controls randomness in predictions. Lower values make predictions more deterministic, while higher values introduce more variability.\n",
        "\n",
        "Let’s see how our model performs text generation!"
      ],
      "metadata": {
        "id": "-0G9TgQg1Qmu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text and map tokens back to characters\n",
        "start_tokens = [token_to_id[ch] for ch in \"ABC\"]  # Example start sequence\n",
        "\n",
        "generated_sequence = model.generate(start_tokens, max_length=10)\n",
        "decoded_sequence = [id_to_token[token] for token in generated_sequence]\n",
        "print(\"Generated sequence:\", \" \".join(decoded_sequence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJ1xX78_03Hc",
        "outputId": "5772211f-0380-4e0b-8ddd-7df8cf4c5df5"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated sequence: C D E F G H I J K L M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that our model has successfully learned the English alphabet, and can generate the next n letters (n = max_seq_length) given a letter sequence.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BhTlsq-f5k_a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercises: Alphabet Sequences\n",
        "\n",
        "1. **Experiment with Sequence Lengths**:\n",
        "   Train the model with longer or shorter sequence lengths. Does the model still generalize well?\n",
        "\n",
        "2. **Add Positional Noise**:\n",
        "   Introduce random shuffling in the input sequences. Observe how the model learns when the sequences are not perfectly ordered.\n",
        "\n",
        "3. **Multi-character Prediction**:\n",
        "   Extend the model to predict pairs of letters (e.g., input: `A B`, output: `C D`).\n",
        "\n",
        "4. **Custom Tokenizer**:\n",
        "   Modify the tokenizer to include lowercase letters or additional symbols.\n",
        "\n",
        "5. **Overfit Small Dataset**:\n",
        "   Train the model on a very small dataset (e.g., `A B C`, `B C D`) and observe its ability to memorize sequences.\n"
      ],
      "metadata": {
        "id": "ba_byg1O6Bag"
      }
    }
  ]
}