{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Word2Vec: Word Embeddings for Natural Language Processing\n",
        "\n",
        "In this notebook, we will explore **Word2Vec**, a powerful technique for learning word embeddings. Word embeddings are dense vector representations of words that capture their semantic meaning and relationships.\n",
        "\n",
        "## Objectives\n",
        "By the end of this notebook, you will:\n",
        "1. Understand the basics of Word2Vec and its two main training approaches: CBOW and Skip-gram.\n",
        "2. Learn how to train a Word2Vec model using the `gensim` library.\n",
        "3. Perform tasks like finding similar words and solving word analogies.\n",
        "4. Save and load the trained Word2Vec model.\n",
        "\n",
        "---\n",
        "\n",
        "## What is Word2Vec?\n",
        "Word2Vec is a technique developed by Google to learn word embeddings from a large corpus of text. These embeddings are vectors in a high-dimensional space, where similar words are close to each other.\n",
        "\n",
        "### Training Approaches:\n",
        "1. **CBOW (Continuous Bag of Words)**:\n",
        "   - Predict a target word from its surrounding context words.\n",
        "   - Faster but less sensitive to rare words.\n",
        "2. **Skip-gram**:\n",
        "   - Predict context words given a target word.\n",
        "   - Slower but better for representing rare words.\n"
      ],
      "metadata": {
        "id": "qGl_5Ts1MVQy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preliminaries: Libraries and Data Preparation\n",
        "\n",
        "We will use the following libraries:\n",
        "1. **`gensim`**: For training and using the Word2Vec model.\n",
        "2. **`nltk`**: To access the `brown` corpus for training data.\n",
        "3. **`nltk.corpus.brown`**: A collection of text from a wide variety of genres.\n",
        "\n",
        "The Brown Corpus is pre-tokenized, making it an ideal dataset for training Word2Vec models.\n",
        "\n"
      ],
      "metadata": {
        "id": "xpEtuaKIMgT5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3BKsZOWXMSk5"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "\n",
        "# Download the Brown Corpus\n",
        "nltk.download('brown')\n",
        "\n",
        "# Load the sentences from the Brown Corpus\n",
        "sentences = brown.sents()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print an example sentences from the corpus\n",
        "print(sentences[:1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0ahmHyhMvjT",
        "outputId": "15466fe6-e6ad-45fc-aaf0-dc72ab744189"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Train a Word2Vec Model\n",
        "\n",
        "We will use the `Word2Vec` class from `gensim` to train a model on the Brown Corpus.\n",
        "\n",
        "### Key Parameters:\n",
        "- **`sentences`**: Input sentences for training.\n",
        "- **`vector_size`**: Dimensionality of the word vectors (default: 100).\n",
        "- **`window`**: Context window size (default: 5).\n",
        "- **`min_count`**: Ignores words that appear fewer than this number of times.\n",
        "- **`workers`**: Number of CPU threads to use.\n",
        "- **`sg`**: Training algorithm (0 for CBOW, 1 for Skip-gram).\n",
        "\n",
        "We will train the model using the CBOW approach (`sg=0`).\n"
      ],
      "metadata": {
        "id": "Ph0aoDC5NTs-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the Word2Vec model\n",
        "model = Word2Vec(\n",
        "    sentences=sentences,\n",
        "    vector_size=100,\n",
        "    window=5,\n",
        "    min_count=1,\n",
        "    workers=4,\n",
        "    sg=0  # 0 for CBOW, 1 for Skip-gram\n",
        ")\n",
        "\n",
        "print(\"Model training completed.\")\n",
        "print(\"\\nVocabulary size:\", len(model.wv.key_to_index))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sY02e85BNMjX",
        "outputId": "e8e38bca-e655-439a-ed40-143eb9df00ae"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model training completed.\n",
            "\n",
            "Vocabulary size: 56057\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Find Similar Words\n",
        "\n",
        "Using the trained Word2Vec model, we can find words that are semantically similar to a given word based on cosine similarity in the embedding space.\n",
        "\n",
        "### Example:\n",
        "If we query for similar words to \"king\", the output might include \"queen\", \"prince\", and \"monarch\" with similarity scores.\n"
      ],
      "metadata": {
        "id": "G4Rzl9ePNoTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to find similar words\n",
        "def find_similar_words(word, topn=5):\n",
        "    try:\n",
        "        similar_words = model.wv.most_similar(word, topn=topn)\n",
        "        print(f\"Words most similar to '{word}':\")\n",
        "        for w, score in similar_words:\n",
        "            print(f\"  {w}: {score:.4f}\")\n",
        "    except KeyError:\n",
        "        print(f\"'{word}' not in vocabulary\")\n",
        "\n",
        "find_similar_words(\"king\")\n",
        "find_similar_words(\"queen\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEu-0ZviNdE-",
        "outputId": "b4ce41fb-4bf6-4b9b-87ac-6c3303fe658f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words most similar to 'king':\n",
            "  Yankee: 0.9662\n",
            "  Model: 0.9661\n",
            "  former: 0.9657\n",
            "  Prince: 0.9651\n",
            "  mood: 0.9650\n",
            "Words most similar to 'queen':\n",
            "  Book: 0.9511\n",
            "  captain: 0.9480\n",
            "  clerk: 0.9469\n",
            "  minister: 0.9464\n",
            "  Lord: 0.9459\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try another example:"
      ],
      "metadata": {
        "id": "fmdW6qGVNz2S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "find_similar_words(\"doctor\")\n",
        "find_similar_words(\"science\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbHE01E_NzUi",
        "outputId": "e320d7ec-c1ad-4210-e6c3-1d42c2c4c857"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words most similar to 'doctor':\n",
            "  boy: 0.9541\n",
            "  patient: 0.9418\n",
            "  President: 0.9378\n",
            "  conversation: 0.9293\n",
            "  letter: 0.9228\n",
            "Words most similar to 'science':\n",
            "  distinction: 0.9684\n",
            "  philosophy: 0.9649\n",
            "  adolescent: 0.9562\n",
            "  style: 0.9531\n",
            "  inherent: 0.9517\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that our model does do a decent job at generating a handful of semantically similar words!\n"
      ],
      "metadata": {
        "id": "d5UPMrXaOWSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercises: Hands-on Practice\n",
        "\n",
        "1. **Train with Skip-gram**:\n",
        "   Modify the training code to use Skip-gram (`sg=1`) instead of CBOW. Compare the results for word similarity and analogy tasks.\n",
        "\n",
        "2. **Experiment with Parameters**:\n",
        "   Change the `window` size or `min_count` parameter in the training process. Observe how these changes affect the vocabulary and results.\n",
        "\n",
        "3. **Out-of-Vocabulary Words**:\n",
        "   Test the model with words not present in the Brown Corpus. How does the model handle such cases?\n",
        "\n",
        "4. **Custom Corpus**:\n",
        "   Train the Word2Vec model on a custom text corpus, such as a collection of articles or books.\n",
        "\n"
      ],
      "metadata": {
        "id": "h9aqvQ8-O6wy"
      }
    }
  ]
}