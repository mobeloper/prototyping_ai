{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuiOQnX8NPNi"
      },
      "source": [
        "# **Latent Dirichlet Allocation (LDA)**\n",
        "Prior to the explosion of interest in Generative AI, there were a set of models specific to NLP & Computer Vision that leveraged generative models. These models are called generative as they are probabilistic in nature. These probabilisitic machine learning models can be made to generate data as they are based on underlying probability distributions. LDA is one such algorithm. It is a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics [1]. There is an assumption of a set of topic probabilities that are unknown and hence referred to as latent. Each topic is modeled as an infinite mixture over these latent topic probabilities. Learning the topic probabilities would be equivalent to the representation of the document as we would know what topics are present and in which proportions. Therefore, this popular machine learning technique is used in topic modeling to discover hidden topics within a collection of documents.\n",
        "\n",
        "### 1. **Unsupervised Technique**\n",
        "   - The key assumption is that any document is a mixture of several topics. Each topic is a distribution of words. For example, in a collection of articles, topics could be \"politics,\" \"technology,\" \"sports,\" etc. For example, an article about artificial intelligence might be 60% \"technology\" and 40% \"business.\"\n",
        "\n",
        "### 2. **How does it Work?**\n",
        "   - **Topics as Word Distributions**: Each topic is represented as a distribution of words. For example, a \"technology\" topic might have high probabilities for words like \"computer,\" \"AI,\" and \"data.\"\n",
        "   - **Probabilistic Model**: LDA is based on Dirichlet distributions (hence the name), which are used to model the probability of topics within a document and words within a topic.\n",
        "\n",
        "### 3. **LDA Steps**\n",
        "   - **Input**: A set of documents and the number of topics \\( K \\) you want to discover.\n",
        "   - **Output**: Probabilities of topics per document and probabilities of words per topic.\n",
        "   - **Algorithm Steps**:\n",
        "     1. Initialize the model with a random assignment of topics to words in documents.\n",
        "     2. Iterate through the words and update topic assignments based on probabilities.\n",
        "     3. Repeat the process until convergence, where topics stabilize, revealing word distributions per topic and topic distributions per document.\n",
        "\n",
        "### 4. **Applications of LDA**\n",
        "   - **Document Clustering**: Grouping similar documents based on topics.\n",
        "   - **Information Retrieval**: Improving search by identifying relevant topics within documents.\n",
        "   - **Content Recommendation**: Recommending content based on similar topics.\n",
        "   - **Text Summarization**: Summarizing content by topic relevance.\n",
        "\n",
        "\n",
        "LDA provides insights into large text datasets by identifying common themes and structures, making it an essential tool for natural language processing (NLP).\n",
        "\n",
        "![LDA](\"/content/nltk/LDAupdated2.png\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7bQWnQHQHGF"
      },
      "source": [
        "\n",
        "Using the popular libraries `gensim` and `nltk` for topic modeling on a simple set of example documents.\n",
        "\n",
        "We'll go through:\n",
        "1. Importing necessary libraries.\n",
        "2. Preprocessing text data.\n",
        "3. Creating and training the LDA model.\n",
        "4. Displaying the topics generated by the model.\n",
        "\n",
        "## Import necessary libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hMW8Tl9P6-3"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BslAAjSeMoFI"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/oysterable/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import gensim\n",
        "from gensim import corpora\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.data.path.append(\"/content/nltk\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdQHhys4O3xA"
      },
      "source": [
        "## Download NLTK Resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LX-UHrINOIyu",
        "outputId": "25d325ef-5a16-461e-e240-d3078a5dfa23"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /content/nltk...\n"
          ]
        },
        {
          "ename": "OSError",
          "evalue": "[Errno 30] Read-only file system: '/content'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpunkt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/content/nltk\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstopwords\u001b[39m\u001b[38;5;124m'\u001b[39m, download_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/content/nltk\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwordnet\u001b[39m\u001b[38;5;124m'\u001b[39m, download_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/content/nltk\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/nltk/downloader.py:777\u001b[0m, in \u001b[0;36mDownloader.download\u001b[0;34m(self, info_or_id, download_dir, quiet, force, prefix, halt_on_error, raise_on_error, print_error_to)\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(s, prefix2\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    769\u001b[0m     print_to(\n\u001b[1;32m    770\u001b[0m         textwrap\u001b[38;5;241m.\u001b[39mfill(\n\u001b[1;32m    771\u001b[0m             s,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    774\u001b[0m         )\n\u001b[1;32m    775\u001b[0m     )\n\u001b[0;32m--> 777\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincr_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43minfo_or_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    778\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Error messages\u001b[39;49;00m\n\u001b[1;32m    779\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mErrorMessage\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessage\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/nltk/downloader.py:642\u001b[0m, in \u001b[0;36mDownloader.incr_download\u001b[0;34m(self, info_or_id, download_dir, force)\u001b[0m\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m FinishCollectionMessage(info)\n\u001b[1;32m    640\u001b[0m \u001b[38;5;66;03m# Handle Packages (delegate to a helper function).\u001b[39;00m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 642\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_package(info, download_dir, force)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/nltk/downloader.py:699\u001b[0m, in \u001b[0;36mDownloader._download_package\u001b[0;34m(self, info, download_dir, force)\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[38;5;66;03m# Ensure the download_dir exists\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(download_dir):\n\u001b[0;32m--> 699\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdownload_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(download_dir, info\u001b[38;5;241m.\u001b[39msubdir)):\n\u001b[1;32m    701\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(download_dir, info\u001b[38;5;241m.\u001b[39msubdir))\n",
            "File \u001b[0;32m<frozen os>:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
            "File \u001b[0;32m<frozen os>:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno 30] Read-only file system: '/content'"
          ]
        }
      ],
      "source": [
        "nltk.download('punkt', download_dir=\"/content/nltk\")\n",
        "nltk.download('stopwords', download_dir=\"/content/nltk\")\n",
        "nltk.download('wordnet', download_dir=\"/content/nltk\")\n",
        "nltk.download('punkt_tab', download_dir=\"/content/nltk\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yi0eUO7vPed3"
      },
      "source": [
        "## Create an Example Set of documents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "uCVi-IqmPDT8"
      },
      "outputs": [],
      "source": [
        "documents = [\n",
        "    \"I love reading about machine learning and natural language processing.\",\n",
        "    \"Artificial intelligence is a fascinating field.\",\n",
        "    \"Deep learning is a part of machine learning.\",\n",
        "    \"I enjoy creating machine learning models.\",\n",
        "    \"Natural language processing is a key component of artificial intelligence.\",\n",
        "    \"Learning about AI and ML is exciting and challenging.\",\n",
        "    \"Generative models are a powerful tool in deep learning.\",\n",
        "    \"Applications of AI include image and speech recognition.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIDBCTAGPlXn"
      },
      "source": [
        "## Preprocess the documents\n",
        "\n",
        "* Tokenize,\n",
        "* remove stopwords,\n",
        "* and lemmatize\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "1rBefqBfPDY3"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words(\"english\"))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess(doc):\n",
        "    tokens = word_tokenize(doc.lower())\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha() and token not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "processed_docs = [preprocess(doc) for doc in documents]\n",
        "processed_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeXGkKoOPUzQ"
      },
      "source": [
        "## Create a dictionary and corpus for LDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "zcsKxEf7PJe-"
      },
      "outputs": [],
      "source": [
        "dictionary = corpora.Dictionary(processed_docs)\n",
        "corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOQDYEEPPSyy"
      },
      "source": [
        "## Train LDA model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "dX3xZKWXPJhd"
      },
      "outputs": [],
      "source": [
        "lda_model = gensim.models.LdaModel(corpus, num_topics=3, id2word=dictionary, passes=15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiSkAabdPQJS"
      },
      "source": [
        "## Display topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Eb_FLPyPJmN",
        "outputId": "2f1b0242-ba4a-4100-cdd9-6b82ab9fc3b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Topics generated by LDA:\n",
            "Topic 1: 0.121*\"learning\" + 0.048*\"machine\" + 0.048*\"deep\" + 0.048*\"model\"\n",
            "Topic 2: 0.061*\"artificial\" + 0.061*\"intelligence\" + 0.061*\"natural\" + 0.061*\"processing\"\n",
            "Topic 3: 0.093*\"learning\" + 0.054*\"ai\" + 0.053*\"application\" + 0.053*\"speech\"\n"
          ]
        }
      ],
      "source": [
        "topics = lda_model.print_topics(num_words=4)\n",
        "print(\"Topics generated by LDA:\")\n",
        "for idx, topic in topics:\n",
        "    print(f\"Topic {idx + 1}: {topic}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYUGEQwkOJ2y"
      },
      "source": [
        "\n",
        "### Explanation\n",
        "\n",
        "1. **Text Preprocessing:** We remove stopwords, tokenize, and lemmatize words to make the text simpler and more relevant for topic modeling.\n",
        "   \n",
        "2. **Creating the Dictionary and Corpus:** `dictionary` maps each word to a unique ID, and `corpus` represents each document as a \"bag of words\" (word frequency counts).\n",
        "\n",
        "3. **Training the LDA Model:** We specify `num_topics=3` to create three topics and use `passes=15` for model optimization. This can be adjusted based on the data.\n",
        "\n",
        "4. **Displaying Topics:** The model will output the top words in each topic. The topics can be interpreted based on the most frequent words found.\n",
        "\n",
        "### Expected Output (Sample)\n",
        "```plaintext\n",
        "Topics generated by LDA:\n",
        "Topic 1: 0.200*\"learning\" + 0.100*\"machine\" + 0.080*\"deep\" + 0.080*\"model\"\n",
        "Topic 2: 0.150*\"intelligence\" + 0.120*\"natural\" + 0.110*\"language\" + 0.100*\"processing\"\n",
        "Topic 3: 0.250*\"ai\" + 0.140*\"application\" + 0.100*\"image\" + 0.090*\"recognition\"\n",
        "```\n",
        "\n",
        "This example shows that the model has learned three distinct topics, roughly centered on:\n",
        "1. Machine learning and deep learning.\n",
        "2. Natural language processing.\n",
        "3. AI applications like image and speech recognition.\n",
        "\n",
        "This code can be extended to larger datasets by changing the `documents` list to a more extensive corpus of text or through a more complex structure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6YxoMINNrLU"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgWm2_IXNmf_"
      },
      "source": [
        "## References\n",
        "\n",
        "[1] Latent Dirichlet Allocation, https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbM7SPFOMnJV"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
