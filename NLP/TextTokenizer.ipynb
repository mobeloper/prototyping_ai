{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original text: I love Python! üòä #coding\n",
      "Tokens: ['I', 'love', 'Python', '!', 'üòä', '#', 'coding']\n",
      "Tokens with types:\n",
      "  I: WORD\n",
      "  love: WORD\n",
      "  Python: WORD\n",
      "  !: PUNCTUATION\n",
      "  üòä: OTHER\n",
      "  #: SPECIAL\n",
      "  coding: WORD\n",
      "\n",
      "Original text: Let's meet at 5pm #meeting üïî\n",
      "Tokens: ['Let', \"'\", 's', 'meet', 'at', '5pm', '#', 'meeting', 'üïî']\n",
      "Tokens with types:\n",
      "  Let: WORD\n",
      "  ': PUNCTUATION\n",
      "  s: WORD\n",
      "  meet: WORD\n",
      "  at: WORD\n",
      "  5pm: WORD\n",
      "  #: SPECIAL\n",
      "  meeting: WORD\n",
      "  üïî: OTHER\n",
      "\n",
      "Original text: Stay positive! üëç #motivation\n",
      "Tokens: ['Stay', 'positive', '!', 'üëç', '#', 'motivation']\n",
      "Tokens with types:\n",
      "  Stay: WORD\n",
      "  positive: WORD\n",
      "  !: PUNCTUATION\n",
      "  üëç: OTHER\n",
      "  #: SPECIAL\n",
      "  motivation: WORD\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "class TextTokenizer:\n",
    "    \"\"\"An enhanced text tokenizer that handles various text elements including words,\n",
    "    contractions, numbers, punctuation, and special characters, using named regex groups.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Token specification with named groups\n",
    "    TOKEN_SPECIFICATION = [\n",
    "        ('NUMBER',      r'\\b\\d+(?:\\.\\d+)?(?:[eE][+-]?\\d+)?\\b'),      # Integer or decimal number\n",
    "        ('WORD',        r'\\b\\w+(?:-\\w+)*\\b'),                        # Words (including hyphenated)\n",
    "        ('CONTRACTION', r\"\\b\\w+'(?:\\w+)?\\b\"),                        # Contractions like can't, don't\n",
    "        ('PUNCTUATION', r'[.,!?;:\"\\'()\\[\\]{}]'),                     # Punctuation\n",
    "        ('SPECIAL',     r'[$%&@#^*+=<>~`|\\\\/]'),                     # Special characters\n",
    "        ('OTHER',       r'\\S'),                                       # Any non-whitespace character\n",
    "    ]\n",
    "    \n",
    "    def __init__(self, custom_patterns: Optional[List[Tuple[str, str]]] = None):\n",
    "        \"\"\"Initialize the tokenizer with optional custom patterns.\n",
    "        \n",
    "        Args:\n",
    "            custom_patterns (list of tuples, optional): List of (name, pattern) tuples\n",
    "                to include in tokenization.\n",
    "        \"\"\"\n",
    "        self.token_specification = self.TOKEN_SPECIFICATION.copy()\n",
    "        if custom_patterns:\n",
    "            self.token_specification.extend(custom_patterns)\n",
    "        \n",
    "        # Combine all patterns into a single regex\n",
    "        self.tokenizer_pattern = '|'.join('(?P<%s>%s)' % pair for pair in self.token_specification)\n",
    "        self.compiled_pattern = re.compile(self.tokenizer_pattern)\n",
    "    \n",
    "    def tokenize(self, text: str, lowercase: bool = False) -> List[str]:\n",
    "        \"\"\"Tokenize the input text.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Input text to tokenize\n",
    "            lowercase (bool): Whether to convert tokens to lowercase\n",
    "        \n",
    "        Returns:\n",
    "            List[str]: List of tokens\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return []\n",
    "        \n",
    "        if lowercase:\n",
    "            text = text.lower()\n",
    "        \n",
    "        tokens = [match.group() for match in self.compiled_pattern.finditer(text)]\n",
    "        return tokens\n",
    "    \n",
    "    def tokenize_with_types(self, text: str) -> List[Tuple[str, str]]:\n",
    "        \"\"\"Tokenize text and identify the type of each token.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Input text to tokenize\n",
    "        \n",
    "        Returns:\n",
    "            List[tuple]: List of (token, token_type) pairs\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        for match in self.compiled_pattern.finditer(text):\n",
    "            token_type = match.lastgroup\n",
    "            token = match.group()\n",
    "            tokens.append((token, token_type))\n",
    "        return tokens\n",
    "\n",
    "def main():\n",
    "    # Define your custom patterns\n",
    "    custom_patterns = [\n",
    "        ('EMOJI', r'[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF]'),  # Match emojis\n",
    "        ('HASHTAG', r'#\\w+'),                                         # Match hashtags\n",
    "    ]\n",
    "    \n",
    "    # Initialize the tokenizer with custom patterns\n",
    "    tokenizer = TextTokenizer(custom_patterns=custom_patterns)\n",
    "    \n",
    "    # Test cases\n",
    "    test_texts = [\n",
    "        \"I love Python! üòä #coding\",\n",
    "        \"Let's meet at 5pm #meeting üïî\",\n",
    "        \"Stay positive! üëç #motivation\",\n",
    "    ]\n",
    "    \n",
    "    for text in test_texts:\n",
    "        print(\"\\nOriginal text:\", text)\n",
    "        print(\"Tokens:\", tokenizer.tokenize(text))\n",
    "        print(\"Tokens with types:\")\n",
    "        for token, token_type in tokenizer.tokenize_with_types(text):\n",
    "            print(f\"  {token}: {token_type}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
